{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection Jupyter Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is loading your API credentials. This can be done with the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open commant prompt/powershell and navigate to some appropriate directory, set up for data collection. \n",
    "\n",
    "Set your bearer token: `Set BEARER_TOKEN = your_bearer_token`\n",
    "\n",
    "Open jupyter notebook through the command prompt: `Jupyter Notebook`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to import libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following libraries are useful for colouring and styling JSON data. This is useful when viewing JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygments import highlight\n",
    "from pygments.formatters.terminal256 import Terminal256Formatter\n",
    "from pygments.lexers.web import JsonLexer\n",
    "from pygments.style import Style\n",
    "from pygments.token import Token, String, Name\n",
    "\n",
    "class MyStyle(Style):\n",
    "    styles = {\n",
    "        Token.String:     'ansimagenta',\n",
    "        Name:             'ansibrightblue'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reads in the headers to set up Twitter Connection. \n",
    "\n",
    "Note that one can directly paste in the Twitter bearer token here. However, code should not be shared with this token included. It is best practise to save the token in an environment variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_twitter():\n",
    "    # bearer_token = \"your_bearer_token\"\n",
    "    bearer_token = os.environ.get(\"BEARER_TOKEN\")\n",
    "    return {\"Authorization\": \"Bearer {}\".format(bearer_token)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = connect_to_twitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows access to the API. The next step is to build the request for the endpoint to use and the parameters to pass. \n",
    "\n",
    "This API connection uses the new Twitter (v2) API and pulls tweets searched with keywords and from and to certain dates.\n",
    "\n",
    "It is slow for large data for a number of reasons: Every 500 tweets it pauses for 5 seconds to avoid the rate limit of one request a minute or 300 requests every 15 minutes. In addition, after 900 requests it will pause for 15 minutes. This is a little conservative and could be lowered, but it is better to be a little conservative to avoid a rate limit error. Information on rate limits is available here: https://developer.twitter.com/en/docs/twitter-api/rate-limits\n",
    "\n",
    "Note that the tweets come in backwards order each day, therefore I create a list of tweets per day and iterate through it in reverse. This is less efficient than writing each tweet in blocks of 500 as if you've thousands of tweets in a day it requires making that list and a list of users to get the username/ This could be sped up but it is not slow enough to justify it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token   \n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    #print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment out code blocks for all other storms but relevant storm (Ctrl + / all cell contents)\n",
    "\n",
    "Set `storm_name` to the desired storm name. \n",
    "\n",
    "A folder named `storm_name` is created. This is where the data is saved in JSON format. \n",
    "\n",
    "Edit `keyword`to whatever words you wish to collect. Edit and fill in the `other` parameter similarly to describe other criteria for collection. Information on filtering tweets is availavle in the API docs such as https://developer.twitter.com/en/docs/twitter-api/tweets/filtered-stream/integrate/build-a-rule. \n",
    "\n",
    "Adjust `year`, `month`, `start_day`, `end_day` as desired. \n",
    "Can collect daily, or a number of days at once (within a month). \n",
    "As the volume of tweets in a single day can be very large, I would advise searching for one day of data at a time (as the occasional \"Service not available\" error may ruin a whole query after collecting a few million tweets taking from the tweet cap. ). Furthermore, if the volume of data is particularly large some days (e.g. March 1st 2018 - the middle of storm Emma), it may be best to divide up the day into two or more data collections. This can be done by changine the hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ophelia\n",
    "# storm_name = 'Emma'\n",
    "storm_name = 'Ophelia'\n",
    "if not os.path.exists(\"Tweets_\"+storm_name):\n",
    "    os.mkdir(\"Tweets_\"+storm_name)\n",
    "\n",
    "#What you want to search for - keywords.\n",
    "#Note this brings up meteireann quote tweets, replies, mentions also.\n",
    "# keyword =\"(Storm OR meteireann OR Emma OR wind OR gale OR windstorm OR hurricane OR rain OR raining OR rainy OR rainstorms OR rainstorm OR hail OR hailstones OR hailstorm OR hailing OR hale OR snow OR blizzard OR snowstorm)\"\n",
    "keyword =\"(Storm OR meteireann OR Ophelia OR Ofelia OR Opelia OR Opehlia OR stormophelia OR Opheliaireland OR wind OR gale OR windstorm OR hurricane)\"\n",
    "\n",
    "#Also restricting search to tweets Twitter detects to be written in English and not retweets. \n",
    "other = \" lang:en -is:retweet\" \n",
    "\n",
    "#Dates to search between\n",
    "# year =  2018\n",
    "# month = 3 # 2 or 3 depending on day\n",
    "# start_day = end_day = 5 # (25-28)U(1,5)\n",
    "year =  2017\n",
    "month = 10 \n",
    "start_day = end_day = 11 # (Change this to colect data between 11-19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The academic product track allows users to request up to 500 request per page (thereis a maximum of 100 for the standard product track)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_results = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour = 0\n",
    "\n",
    "if month < 10:\n",
    "    month = '0' + str(month)\n",
    "\n",
    "start_day_str = start_day\n",
    "end_day_str = end_day \n",
    "if start_day_str < 10:\n",
    "    start_day_str = '0' + str(start_day_str)\n",
    "if end_day_str < 10:\n",
    "    end_day_str = '0' + str(end_day_str)\n",
    "    \n",
    "start= str(year) +\"-\"+str(month)+\"-\"+str(start_day_str)+\"T00:00:00.000Z\"\n",
    "\n",
    "#Output file name\n",
    "output = open('Tweets_'+storm_name+'/tweets_' + storm_name + '_' + start.replace('-','_').replace(':','_').split('T00')[0] +'.json','w', encoding='utf-8') \n",
    "\n",
    "#Change to the endpoint you want to collect data from\n",
    "\n",
    "#academic product track allows full archive search\n",
    "url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "#academic product track allows recent search\n",
    "#url = \"https://api.twitter.com/2/tweets/search/recent\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search_url is the link of the endpoint we want to access. The query parameters of the endpoint allows customisation of the request. \n",
    "The API-reference page for the full-archive search endpoint query parameters is here: https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate through hours\n",
    "\n",
    "#Tweet counter\n",
    "count = 0\n",
    "\n",
    "#Request counter\n",
    "n_requests = 0\n",
    "\n",
    "my_dict={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/an-extensive-guide-to-collecting-tweets-from-twitter-api-v2-for-academic-research-using-python-3-518fcb71df2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you want to break down the search into a number of searches per day (for say busy periods), this code will have to be altered slightly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the dates\n",
    "\n",
    "for date in range(start_day,end_day+1):\n",
    "    if date < 10:\n",
    "        date = '0' + str(date)\n",
    "    start_date= str(year) +\"-\"+str(month)+\"-\"+str(date)+\"T\"+str(hour)+\":00:00.000Z\"\n",
    "    end_date= str(year) +\"-\"+ str(month)+\"-\"+str(date)+\"T\"+str(hour+23)+\":59:59.000Z\"\n",
    "    \n",
    "    print(start_date.split('T')[0])\n",
    "    \n",
    "    params = {'query': keyword + other,\n",
    "                'start_time': start_date,\n",
    "                'end_time': end_date,\n",
    "                'max_results': max_results,\n",
    "                'expansions': 'author_id,in_reply_to_user_id,geo.place_id,referenced_tweets.id',\n",
    "                'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source,entities',\n",
    "                'user.fields': 'id,name,username,location,created_at,description,public_metrics,verified',\n",
    "                'place.fields': 'full_name,id,country,country_code,geo,name,place_type,contained_within',\n",
    "                'next_token': {}}\n",
    "\n",
    "    ####\n",
    "    #loop through pages of the search\n",
    "\n",
    "    #This first one is arbitrary, defined it to be 0 when there are none\n",
    "    next_token = 1\n",
    "\n",
    "    #as the API gets from the last time to first time, create a list and reverse it later\n",
    "    tweets, users, places, ref_tweets = [], [], [], []\n",
    "\n",
    "    #iterate through next tokens\n",
    "    while next_token != 0:\n",
    "\n",
    "        #count requests made\n",
    "        n_requests += 1\n",
    "        \n",
    "        #This is going to have up to 500 tweets and a next_token if more\n",
    "        json_response = connect_to_endpoint(url, headers, params) if next_token == 1 else  connect_to_endpoint(url, headers, params, next_token) \n",
    "\n",
    "        #Get next_token if there is one\n",
    "        if 'next_token' not in json_response['meta']:\n",
    "            next_token = 0\n",
    "        else:\n",
    "            next_token = json_response['meta']['next_token']\n",
    "\n",
    "        #Get number of tweets\n",
    "        count += json_response['meta']['result_count']\n",
    "        print('Tweets collected:',count, '- Requests:', n_requests)\n",
    "\n",
    "        #We could write here but due to the backwards order I save it to a list instead\n",
    "        tweets += json_response['data']\n",
    "        users += json_response['includes']['users']\n",
    "        if 'places' in json_response['includes']:\n",
    "            places += json_response['includes']['places']\n",
    "        if 'tweets' in json_response['includes']:\n",
    "            ref_tweets += json_response['includes']['tweets']\n",
    "            \n",
    "        #Pause to not hit rate limit\n",
    "        time.sleep(5) \n",
    "\n",
    "        #Every 900 requests you have to wait 15 minutes\n",
    "        if n_requests %900 == 0:\n",
    "            time.sleep(900)\n",
    "\n",
    "    ##############\n",
    "    #Write to file\n",
    "    # Dictionaries:\n",
    "    # USER STUFF\n",
    "    usernames = {u['id']:u['username'] for u in users}\n",
    "    user_location = {u['id']:u['location'] for u in users if 'location' in u}\n",
    "    n_followers = {u['id']:u['public_metrics']['followers_count'] for u in users}\n",
    "    n_following = {u['id']:u['public_metrics']['following_count'] for u in users}\n",
    "    user_description = {u['id']:u['description'] for u in users}\n",
    "    user_entities = {u['id']:u['entities'] for u in users if 'entities' in u}\n",
    "    user_created_at = {u['id']:u['created_at'] for u in users}\n",
    "    \n",
    "    # LOCATION STUFF\n",
    "    country = {p['id']:p['country_code'] for p in places if 'country_code' in p}\n",
    "    place_full_name = {p['id']:p['full_name'] for p in places if 'full_name' in p}\n",
    "    place_name = {p['id']:p['name'] for p in places if 'name' in p}\n",
    "    places_geo = {p['id']:p['geo'] for p in places if 'geo' in p}# geo in data.includes.places\n",
    "    place_type = {p['id']:p['place_type'] for p in places if 'place_type' in p}\n",
    "    \n",
    "    # REFERENCED_TWEET_STUFF\n",
    "    referenced_tweets_text = {t['id']:t['text'] for t in ref_tweets if 'text' in t}\n",
    "    referenced_tweets_author_id = {t['id']:t['author_id'] for t in ref_tweets if 'text' in t}\n",
    "    referenced_tweets_conversation_id = {t['id']:t['conversation_id'] for t in ref_tweets if 'text' in t}\n",
    "    referenced_tweets_created_at = {t['id']:t['created_at'] for t in ref_tweets if 'text' in t}\n",
    "    referenced_tweets_lang = {t['id']:t['lang'] for t in ref_tweets if 'text' in t}\n",
    "    \n",
    "    #loop through tweets in reverse order (NOTE: if you change the hour you'll have order issues here)\n",
    "    for tweet in reversed(tweets):\n",
    "\n",
    "        #Get user ID\n",
    "        uid = tweet['author_id']\n",
    "        \n",
    "        tweet_GPS = tweet['geo']['place_id'] if 'geo' in tweet and 'place_id' in tweet['geo'] else ''\n",
    "        \n",
    "        #Get lists of hashtags and mentions\n",
    "        hasht = []\n",
    "        ment = []\n",
    "        if 'entities' in tweet:\n",
    "            if 'hashtags' in tweet['entities']:\n",
    "                for u in tweet['entities']['hashtags']:\n",
    "                    hasht.append(u['tag'])\n",
    "            if 'mentions' in tweet['entities']:\n",
    "                for u in tweet['entities']['mentions']:\n",
    "                    ment.append(u['username'])\n",
    "\n",
    "        #Check if retweet\n",
    "        RT = 'n'\n",
    "        if 'referenced_tweets' in tweet and tweet['text'][:2] == 'RT':\n",
    "            RT = 'y'\n",
    "        referenced_tweet_id = tweet['referenced_tweets'][0]['id'] if 'referenced_tweets' in tweet else ''\n",
    "        \n",
    "        my_dict_line = {}\n",
    "        my_dict_line['tweet_id']=tweet.get('id')\n",
    "        my_dict_line['conversation_id']=tweet.get('conversation_id')\n",
    "        my_dict_line['author_id']=tweet.get('author_id')\n",
    "        my_dict_line['created_at']=tweet.get('created_at')\n",
    "        my_dict_line['text']=tweet.get('text')\n",
    "        my_dict_line['lang']=tweet.get('lang')\n",
    "        my_dict_line['retweet']=RT\n",
    "        \n",
    "        my_dict_line['username']=usernames[uid]\n",
    "        my_dict_line['user_location']=user_location[uid] if uid in user_location else ''\n",
    "        my_dict_line['n_followers']=n_followers[uid] if uid in n_followers else ''\n",
    "        my_dict_line['n_following']=n_following[uid] if uid in n_following else ''\n",
    "        my_dict_line['user_description']=user_description[uid] if uid in user_description else ''\n",
    "        my_dict_line['user_entities']=user_entities[uid] if uid in user_entities else ''\n",
    "        my_dict_line['user_created_at']=user_created_at[uid] if uid in user_created_at else ''\n",
    "        \n",
    "        my_dict_line['geo']=tweet.get('geo') \n",
    "        my_dict_line['country']=country[tweet_GPS] if 'geo' in tweet and tweet_GPS in country else ''\n",
    "        my_dict_line['place_full_name']=place_full_name[tweet_GPS] if 'geo' in tweet and tweet_GPS in country else ''\n",
    "        my_dict_line['place_name']=place_name[tweet_GPS] if 'geo' in tweet and tweet_GPS in country else ''\n",
    "        my_dict_line['places_geo']=places_geo[tweet_GPS] if 'geo' in tweet and tweet_GPS in country else ''\n",
    "        my_dict_line['place_type']=place_type[tweet_GPS] if 'geo' in tweet and tweet_GPS in country else ''\n",
    "        \n",
    "        my_dict_line['public_metrics']=tweet.get('public_metrics')\n",
    "        my_dict_line['entities']=tweet.get('entities')\n",
    "        my_dict_line['in_reply_to_user_id']=tweet.get('in_reply_to_user_id')\n",
    "        my_dict_line['referenced_tweets']=tweet.get('referenced_tweets')\n",
    "        \n",
    "        my_dict_line['referenced_tweet_id']=referenced_tweet_id\n",
    "        my_dict_line['referenced_tweets_text']=referenced_tweets_text[referenced_tweet_id] if 'referenced_tweets' in tweet and referenced_tweet_id in referenced_tweets_text else ''\n",
    "        my_dict_line['referenced_tweets_author_id']=referenced_tweets_author_id[referenced_tweet_id] if 'referenced_tweets' in tweet and referenced_tweet_id in referenced_tweets_author_id else ''\n",
    "        my_dict_line['referenced_tweets_conversation_id']=referenced_tweets_conversation_id[referenced_tweet_id] if 'referenced_tweets' in tweet and referenced_tweet_id in referenced_tweets_conversation_id else ''\n",
    "        my_dict_line['referenced_tweets_created_at']=referenced_tweets_created_at[referenced_tweet_id] if 'referenced_tweets' in tweet and referenced_tweet_id in referenced_tweets_created_at else ''\n",
    "        my_dict_line['referenced_tweets_lang']=referenced_tweets_lang[referenced_tweet_id] if 'referenced_tweets' in tweet and referenced_tweet_id in referenced_tweets_lang else ''\n",
    "        \n",
    "        output.write(json.dumps(my_dict_line) + '\\n')\n",
    "    #End tweet loop\n",
    "    #############\n",
    "\n",
    "#Close output file\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of requests made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what one request looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response_col = highlight(\n",
    "    json.dumps(json_response, sort_keys=True, indent=4),\n",
    "    lexer=JsonLexer(),\n",
    "    formatter=Terminal256Formatter(style=MyStyle),\n",
    ")\n",
    "print(json_response_col)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
